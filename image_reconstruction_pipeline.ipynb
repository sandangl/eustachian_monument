{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78517d26",
   "metadata": {},
   "source": [
    "# Image Stereoscopic Reconstruction (Pipeline)\n",
    "\n",
    "In this notebook, we explore the process of image translation, in order to obtain a frontal view of an architectural object from the corresponding lateral view, with possible image enhancements (inclusion of new details, inpainting, etc.).\n",
    "To achieve this, we are going to use an attention based, Chain of Thoughts (CoT) driven generative process, which includes an LLM coupled with a Conditional Latent Diffusion Model (in our example, we are using Qwen 2.5 Image Edit)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbcf4a96",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557f7c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fadd053",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import io\n",
    "import json\n",
    "import ollama\n",
    "import chromadb\n",
    "from matplotlib import image as mpimg\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8befed00",
   "metadata": {},
   "source": [
    "### Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ef11f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_image(image_path) -> str:\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "    \n",
    "def decode_and_show_image(base64_img, img_format: str):\n",
    "    decoded_bytes = io.BytesIO(base64.b64decode(base64_img))\n",
    "    decoded_image = mpimg.imread(decoded_bytes, format=img_format)\n",
    "    \n",
    "    plt.imshow(decoded_image, interpolation='nearest')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a608590f",
   "metadata": {},
   "source": [
    "### Ollama\n",
    "\n",
    "We make use of [Ollama](), a local LLM orchestrator.\n",
    "Feel free to experiment with other vision models of your taste ([list of available ones](https://ollama.com/search?c=vision))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bc2a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "OLLAMA_URL = \"http://localhost:11434\"   # Feel free to change if your Ollama port is different\n",
    "MODEL = \"qwen2.5vl:32b\"                 # Our approach is tested with and works best with Qwen2.5-VL 32B.\n",
    "\n",
    "%ollama pull $MODEL\n",
    "%ollama serve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41ee388",
   "metadata": {},
   "source": [
    "### Vector Store\n",
    "\n",
    "We make use of [ChromaDB](https://www.trychroma.com/), a lightweight and easy to set up in-memory vector store.\n",
    "Documentation can be found [here](https://docs.trychroma.com/docs/overview/getting-started)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f81d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_client = chromadb.EphemeralClient()  #By default, we use an in-memory approach which does not persist anything for this demo.\n",
    "collection = chroma_client.create_collection(name=\"eustachian_collection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27b111d",
   "metadata": {},
   "source": [
    "Adding two images to the vector store, for final image details enrichment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05ce783",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.add(\n",
    "    ids=[\"id1\", \"id2\"],\n",
    "    documents=[\n",
    "        f\"\"\"\n",
    "        {{ \n",
    "            \"caption\": \"A statue of St. Eustace, patron saint of Matera, suited in armor with a golden plume, holding a spear upright in its right hand.\",\n",
    "            \"base64\": \"{encode_image('assets/stEustace.jpg')}\"\n",
    "        }}\n",
    "        \"\"\",\n",
    "        f\"\"\"\n",
    "        {{ \n",
    "            \"caption\": \"A statue of St. Vitus, suited in light armor and a red cape, bringing a silver cross in is left hand, followed by two dogs of the same breed, of brown and black.\",\n",
    "            \"base64\": \"{encode_image('assets/stVitus.jpg')}\"\n",
    "        }}\n",
    "        \"\"\"\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd7383b",
   "metadata": {},
   "source": [
    "#### Example of querying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e6ccb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = collection.query(\n",
    "    query_texts=[\"This is a query document about a saint followed by dogs\"],\n",
    "    n_results=1\n",
    ")\n",
    "print(json.loads(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefb9ad8",
   "metadata": {},
   "source": [
    "## Phase 1: Prospective Change\n",
    "\n",
    "To solve this purpose, we make use of Qwen 2.5 Image Edit, an **instructive** T2I model capable of image generation, image editing and in-context image editing, over a CoT-LLM infrastructure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05737bc6",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8ace62",
   "metadata": {},
   "source": [
    "Cloning from public [Qwen Huggingface repo](https://huggingface.co/Qwen/Qwen-Image-Edit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847a4fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/QwenLM/Qwen-Image.git ./models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a059862c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install git+https://github.com/huggingface/diffusers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ac68c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from diffusers import QwenImageEditPlusPipeline\n",
    "from typing import List, Dict\n",
    "from io import BytesIO\n",
    "from ollama import chat, ChatResponse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f73d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = QwenImageEditPlusPipeline.from_pretrained(\"Qwen/Qwen-Image-2509\")\n",
    "print(\"Pipeline loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb372c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.to(torch.bfloat16) # Inference in mixed precision (raccomended for faster inference)\n",
    "pipeline.to(\"cuda\")         # Change to \"cpu\" if CUDA is not supported by your machine. The inference will be slower.\n",
    "pipeline.set_progress_bar_config(disable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8448744e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_image(inputs: Dict[Image.Image, str, torch.Generator, float, str, int, int], output_filename = \"output_edit\"):\n",
    "    with torch.inference_mode():\n",
    "        output = pipeline(**inputs)\n",
    "        i = 0\n",
    "        for output_image in output.images:\n",
    "            output_image.save(f\"{output_filename}_{i}.jpeg\")\n",
    "            i += 1\n",
    "        print(\"Image generated (saved!)\")\n",
    "    return output_image\n",
    "\n",
    "def make_inputs(images: List[Image.Image], prompt: str, guidance = 4.0, neg_prompt = \" \", inf_steps = 50, gen_images = 1):\n",
    "    return {\n",
    "    \"image\": images,\n",
    "    \"prompt\": prompt,\n",
    "    \"generator\": torch.manual_seed(0),\n",
    "    \"true_cfg_scale\": guidance,\n",
    "    \"negative_prompt\": neg_prompt,\n",
    "    \"num_inference_steps\": inf_steps,\n",
    "    \"num_images_per_prompt\": gen_images\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169226eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def polish_prompt_en(original_prompt: str, images: List[Image.Image]):\n",
    "    base64_images = []\n",
    "    for image in images:\n",
    "        with BytesIO() as buffered:\n",
    "            image.save(buffered, format=\"JPEG\")\n",
    "            base64_images.append(base64.b64encode(buffered.getvalue()).decode(\"utf8\"))\n",
    "    response: ChatResponse = chat(model=MODEL, messages=[\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content':  f'''\n",
    "        # Edit Instruction Rewriter\n",
    "        You are a professional edit instruction rewriter. Your task is to generate a precise, concise, and visually achievable professional-level edit instruction based on the user-provided instruction and the image to be edited.  \n",
    "\n",
    "        Please strictly follow the rewriting rules below:\n",
    "\n",
    "        ## 1. General Principles\n",
    "        - Keep the rewritten prompt **concise**. Avoid overly long sentences and reduce unnecessary descriptive language.  \n",
    "        - If the instruction is contradictory, vague, or unachievable, prioritize reasonable inference and correction, and supplement details when necessary.  \n",
    "        - Keep the core intention of the original instruction unchanged, only enhancing its clarity, rationality, and visual feasibility.  \n",
    "        - All added objects or modifications must align with the logic and style of the edited input image’s overall scene.  \n",
    "\n",
    "        ## 2. Task Type Handling Rules\n",
    "        ### 1. Add, Delete, Replace Tasks\n",
    "        - If the instruction is clear (already includes task type, target entity, position, quantity, attributes), preserve the original intent and only refine the grammar.  \n",
    "        - If the description is vague, supplement with minimal but sufficient details (category, color, size, orientation, position, etc.). For example:  \n",
    "            > Original: \"Add an animal\"  \n",
    "            > Rewritten: \"Add a light-gray cat in the bottom-right corner, sitting and facing the camera\"  \n",
    "        - Remove meaningless instructions: e.g., \"Add 0 objects\" should be ignored or flagged as invalid.  \n",
    "        - For replacement tasks, specify \"Replace Y with X\" and briefly describe the key visual features of X.  \n",
    "\n",
    "        ### 2. Text Editing Tasks\n",
    "        - All text content must be enclosed in English double quotes `\" \"`. Do not translate or alter the original language of the text, and do not change the capitalization.  \n",
    "        - **For text replacement tasks, always use the fixed template:**\n",
    "            - `Replace \"xx\" to \"yy\"`.  \n",
    "            - `Replace the xx bounding box to \"yy\"`.  \n",
    "        - If the user does not specify text content, infer and add concise text based on the instruction and the input image’s context. For example:  \n",
    "            > Original: \"Add a line of text\" (poster)  \n",
    "            > Rewritten: \"Add text \\\"LIMITED EDITION\\\" at the top center with slight shadow\"  \n",
    "        - Specify text position, color, and layout in a concise way.  \n",
    "\n",
    "        ### 3. Human Editing Tasks\n",
    "        - Maintain the person’s core visual consistency (ethnicity, gender, age, hairstyle, expression, outfit, etc.).  \n",
    "        - If modifying appearance (e.g., clothes, hairstyle), ensure the new element is consistent with the original style.  \n",
    "        - **For expression changes, they must be natural and subtle, never exaggerated.**  \n",
    "        - If deletion is not specifically emphasized, the most important subject in the original image (e.g., a person, an animal) should be preserved.\n",
    "            - For background change tasks, emphasize maintaining subject consistency at first.  \n",
    "        - Example:  \n",
    "            > Original: \"Change the person’s hat\"  \n",
    "            > Rewritten: \"Replace the man’s hat with a dark brown beret; keep smile, short hair, and gray jacket unchanged\"  \n",
    "\n",
    "        ### 4. Style Transformation or Enhancement Tasks\n",
    "        - If a style is specified, describe it concisely with key visual traits. For example:  \n",
    "            > Original: \"Disco style\"  \n",
    "            > Rewritten: \"1970s disco: flashing lights, disco ball, mirrored walls, colorful tones\"  \n",
    "        - If the instruction says \"use reference style\" or \"keep current style,\" analyze the input image, extract main features (color, composition, texture, lighting, art style), and integrate them into the prompt.  \n",
    "        - **For coloring tasks, including restoring old photos, always use the fixed template:** \"Restore old photograph, remove scratches, reduce noise, enhance details, high resolution, realistic, natural skin tones, clear facial features, no distortion, vintage photo restoration\"  \n",
    "        - If there are other changes, place the style description at the end.\n",
    "\n",
    "        ## 3. Rationality and Logic Checks\n",
    "        - Resolve contradictory instructions: e.g., \"Remove all trees but keep all trees\" should be logically corrected.  \n",
    "        - Add missing key information: if position is unspecified, choose a reasonable area based on composition (near subject, empty space, center/edges).  \n",
    "\n",
    "        # Output Format Example\n",
    "        \"211 floors high skyscrape, majesticly dominating the crowded street below...\"\n",
    "\n",
    "        Prompt to be rewritten: {original_prompt}\n",
    "        ''',\n",
    "        \"images\": base64_images\n",
    "    }\n",
    "    ])\n",
    "    return response['message']['content']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec96e643",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91856cd",
   "metadata": {},
   "source": [
    "#### Solid Rotation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50fa1c6",
   "metadata": {},
   "source": [
    "We take a ROI of the image beforehand, including just the monument for better results for the rotation task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58371a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_full = Image.open(\"assets/eustachian_monument.jpeg\").convert(\"RGB\")\n",
    "image_roi = image_full.crop((200, 500, 800, 1100))\n",
    "image_roi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e44a8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = [image_full]\n",
    "prompt = polish_prompt_en(\"Rotate the main subject, so it looks in front view.\", images)\n",
    "inputs = make_inputs(images, prompt)\n",
    "output_image = generate_image(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b37000",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc837642",
   "metadata": {},
   "source": [
    "## Phase 2: Inpainting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5d9473",
   "metadata": {},
   "source": [
    "## Final Result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62de4e5",
   "metadata": {},
   "source": [
    "### Further improvements\n",
    "\n",
    "These operations aim to edit the previous image in order to have better quality images, but are not strictly necessary.\n",
    "You can skip these steps if you wish."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ffb9b2",
   "metadata": {},
   "source": [
    "#### Colorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa1fb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(\"./output_edit.jpeg\").convert(\"RGB\")\n",
    "prompt = \"Colorize the image.\"\n",
    "inputs = make_inputs(image, prompt)\n",
    "output_image = generate_image(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e373298",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_image"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eustachian_monument",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
