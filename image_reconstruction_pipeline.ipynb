{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78517d26",
   "metadata": {},
   "source": [
    "# Image Stereoscopic Reconstruction (Pipeline)\n",
    "\n",
    "In this notebook, we explore the process of image translation, in order to obtain a frontal view of an architectural object from the corresponding lateral view, with possible image enhancements (inclusion of new details, inpainting, etc.).\n",
    "To achieve this, we are going to use an attention based, Chain of Thoughts (CoT) driven generative process, which includes an LLM coupled with a Conditional Latent Diffusion Model (in our example, we are using Qwen 2.5 Image Edit)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbcf4a96",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557f7c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fadd053",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import io\n",
    "import json\n",
    "import ollama\n",
    "import chromadb\n",
    "from matplotlib import image as mpimg\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8befed00",
   "metadata": {},
   "source": [
    "### Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ef11f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_image(image_path) -> str:\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "    \n",
    "def decode_and_show_image(base64_img, img_format: str):\n",
    "    decoded_bytes = io.BytesIO(base64.b64decode(base64_img))\n",
    "    decoded_image = mpimg.imread(decoded_bytes, format=img_format)\n",
    "    \n",
    "    plt.imshow(decoded_image, interpolation='nearest')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a608590f",
   "metadata": {},
   "source": [
    "### Ollama\n",
    "\n",
    "We make use of [Ollama](), a local LLM orchestrator.\n",
    "Feel free to experiment with other vision models of your taste ([list of available ones](https://ollama.com/search?c=vision))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bc2a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "OLLAMA_URL = \"http://localhost:11434\"   # Feel free to change if your Ollama port is different\n",
    "MODEL = \"qwen2.5vl:32b\"                 # Our approach is tested with and works best with Qwen2.5-VL 32B.\n",
    "\n",
    "%ollama pull $MODEL\n",
    "%ollama serve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41ee388",
   "metadata": {},
   "source": [
    "### Vector Store\n",
    "\n",
    "We make use of [ChromaDB](https://www.trychroma.com/), a lightweight and easy to set up in-memory vector store.\n",
    "Documentation can be found [here](https://docs.trychroma.com/docs/overview/getting-started)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f81d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_client = chromadb.EphemeralClient()  #By default, we use an in-memory approach which does not persist anything for this demo.\n",
    "collection = chroma_client.create_collection(name=\"eustachian_collection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27b111d",
   "metadata": {},
   "source": [
    "Adding two images to the vector store, for final image details enrichment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05ce783",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.add(\n",
    "    ids=[\"id1\", \"id2\"],\n",
    "    documents=[\n",
    "        f\"\"\"\n",
    "        {{ \n",
    "            \"caption\": \"A statue of St. Eustace, patron saint of Matera, suited in armor with a golden plume, holding a spear upright in its right hand.\",\n",
    "            \"base64\": \"{encode_image('assets/stEustace.jpg')}\"\n",
    "        }}\n",
    "        \"\"\",\n",
    "        f\"\"\"\n",
    "        {{ \n",
    "            \"caption\": \"A statue of St. Vitus, suited in light armor and a red cape, bringing a silver cross in is left hand, followed by two dogs of the same breed, of brown and black.\",\n",
    "            \"base64\": \"{encode_image('assets/stVitus.jpg')}\"\n",
    "        }}\n",
    "        \"\"\"\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd7383b",
   "metadata": {},
   "source": [
    "#### Example of querying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e6ccb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = collection.query(\n",
    "    query_texts=[\"This is a query document about a saint followed by dogs\"],\n",
    "    n_results=1\n",
    ")\n",
    "print(json.loads(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefb9ad8",
   "metadata": {},
   "source": [
    "## Phase 1: Prospective Change\n",
    "\n",
    "To solve this purpose, we make use of Qwen 2.5 Image Edit, an **instructive** T2I model capable of image generation, image editing and in-context image editing, over a CoT-LLM infrastructure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05737bc6",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8ace62",
   "metadata": {},
   "source": [
    "Cloning from public [Qwen Huggingface repo](https://huggingface.co/Qwen/Qwen-Image-Edit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847a4fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://huggingface.co/spaces/Qwen/Qwen-Image-Edit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a059862c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install git+https://github.com/huggingface/diffusers\n",
    "%pip install -r Qwen-Image-Edit/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ac68c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from diffusers import QwenImageEditPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f73d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = QwenImageEditPipeline.from_pretrained(\"Qwen/Qwen-Image-Edit\")\n",
    "print(\"Pipeline loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb372c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.to(torch.bfloat16) # Inference in mixed precision (raccomended for faster inference)\n",
    "pipeline.to(\"cuda\")         # Change to \"cpu\" if CUDA is not supported by your machine. The inference will be slower.\n",
    "pipeline.set_progress_bar_config(disable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8448744e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_image(inputs):\n",
    "    with torch.inference_mode():\n",
    "        output = pipeline(**inputs)\n",
    "        output_image = output.images[0]\n",
    "        output_image.save(\"output_edit.jpeg\")\n",
    "        print(\"Image generated (saved!)\")\n",
    "    return output_image\n",
    "\n",
    "def make_inputs(image: Image, prompt: str, neg_prompt = \" \"):\n",
    "    return {\n",
    "    \"image\": image,\n",
    "    \"prompt\": prompt,\n",
    "    \"generator\": torch.manual_seed(0),\n",
    "    \"true_cfg_scale\": 4.0,\n",
    "    \"negative_prompt\": neg_prompt,\n",
    "    \"num_inference_steps\": 50,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec96e643",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91856cd",
   "metadata": {},
   "source": [
    "#### Solid Rotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e44a8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(\"./.jpeg\").convert(\"RGB\")\n",
    "prompt = \"Make the main subject appear in front view.\"\n",
    "inputs = make_inputs(image, prompt)\n",
    "output_image = generate_image(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b37000",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62de4e5",
   "metadata": {},
   "source": [
    "### Further improvements\n",
    "\n",
    "These operations aim to edit the previous image in order to have better quality images, but are not strictly necessary.\n",
    "You can skip these steps if you wish."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ffb9b2",
   "metadata": {},
   "source": [
    "#### Colorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa1fb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(\"./output_edit.jpeg\").convert(\"RGB\")\n",
    "prompt = \"Colorize the image.\"\n",
    "inputs = make_inputs(image, prompt)\n",
    "output_image = generate_image(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e373298",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0ccf96",
   "metadata": {},
   "source": [
    "#### Resolution Upscaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f665509",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(\"./output_edit.jpeg\").convert(\"RGB\")\n",
    "prompt = \"Enhance the resolution of the image.\"\n",
    "inputs = make_inputs(image, prompt)\n",
    "output_image = generate_image(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29209a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc837642",
   "metadata": {},
   "source": [
    "## Phase 2: Image RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5d9473",
   "metadata": {},
   "source": [
    "## Final Result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
